<!DOCTYPE html>
<html>
    <head>
        <title>Shichen Liu's Homepage</title>
        <link rel="stylesheet" href="static/css/stylesheet.css" />
        <link rel="stylesheet" href="static/font-awesome-4.6.3/css/font-awesome.min.css" />
    </head>
    <body>
        <div class="col-3 nav">
            <a href="#"><img src="media/profile.jpg" class="img-circle" width="40%" /></a>
            <h1>Shichen Liu</h1>
            <p>Ph. D. Student of</p>
            <p>Computer Science</p>
            <p><a href="https://www.usc.edu"><i class="fa fa-university" aria-hidden="true"></i> University of Southern California</a></p>
            <p class="icon"><a href="http://github.com/shichenliu"><i class="fa fa-github" aria-hidden="true"></i></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="mailto:liushichen95@gmail.com"><i class="fa fa-envelope" aria-hidden="true"></i></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.facebook.com/shichen.liu.75"><i class="fa fa-facebook-square" aria-hidden="true"></i></a><p>
        </div>
        <div class="col-9 col-offset-3">
            <div class="bio">
                <h1><i class="fa fa-paperclip" aria-hidden="true"></i> About me</h1>
                <p>I am a first year Ph. D. student in the University of Southern California, under the supervision of Professor <a href="http://www.hao-li.com/Hao_Li/Hao_Li_-_about_me.html" target="_blank">Hao Li</a>. Previously, I was in the Machine Learning Group in School of Software, Tsinghua University, under the supervision of Professor <a href="http://ise.thss.tsinghua.edu.cn/~mlong/" target="_blank">Mingsheng Long</a>. I spent 12 wonderful weeks as research assistant in <a href="http://www.cs.cornell.edu/~kilian/index.html" target="_blank">Prof. Kilian Q. Weinberger</a>'s lab in Cornell University during the summer of 2017, working with Postdoc <a href="http://www.gaohuang.net/" target="_blank">Gao Huang</a>. I am currently focusing on unsupervised 3D object reconstruction through rendering. I am also interested in transfer learning and neural architecture design. </p> 

                Here is my <a href="media/cv_lsc.pdf" target="_blank">CV</a>.
            </div>
            <div class="paper-list">
                <h1><i class="fa fa-file-pdf-o" aria-hidden="true"></i> Papers</h1>
                <ul>
                    <li><div class="paper" target="softras">
                        <a class="paper-title" href="https://arxiv.org/abs/1901.05567" target="_blank">Soft Rasterizer: Differentiable Rendering for Unsupervised Single-View Mesh Reconstruction</a> <br />
                        <span class="paper-author"><u>Shichen Liu</u>, Weikai Chen, Tianye Li, Hao Li</span> <br />
                        <span class="paper-conf">Arxiv Preprint</span> <br />
                        </div>
                        <div class="paper-img" id="softras"><img src="media/softras.png" /> <div><span class="paper-abstract">Abstract</span>: Rendering is the process of generating 2D images from 3D assets, simulated in a virtual environment, typically with a graphics pipeline. By inverting such renderer, one can think of a learning approach to predict a 3D shape from an input image. However, standard rendering pipelines involve a fundamental discretization step called rasterization, which prevents the rendering process to be differentiable, hence suitable for learning. We present the first non-parametric and truly differentiable rasterizer based on silhouettes. Our method enables unsupervised learning for high-quality 3D mesh reconstruction from a single image. We call our framework `soft rasterizer' as it provides an accurate soft approximation of the standard rasterizer. The key idea is to fuse the probabilistic contributions of all mesh triangles with respect to the rendered pixels. When combined with a mesh generator in a deep neural network, our soft rasterizer is able to generate an approximated silhouette of the generated polygon mesh in the forward pass. The rendering loss is back-propagated to supervise the mesh generation without the need of 3D training data. Experimental results demonstrate that our approach significantly outperforms the state-of-the-art unsupervised techniques, both quantitatively and qualitatively. We also show that our soft rasterizer can achieve comparable results to the cutting-edge supervised learning method and in various cases even better ones, especially for real-world data.</div></div>
                    </li>


                    <li><div class="paper" target="deep-network-calibration">
                        <a class="paper-title" href="https://papers.nips.cc/paper/7471-generalized-zero-shot-learning-with-deep-calibration-network.pdf" target="_blank">Generalized Zero-Shot Learning with Deep Calibration Network</a> <br />
                        <span class="paper-author"><u>Shichen Liu</u>, Mingsheng Long, Jianmin Wang, Michael I. Jordan</span> <br />
                        <span class="paper-conf">Neural Information Processing Systems (<span style="font-weight:bold">NeurIPS</span>) 2018</span> <br />
                        </div>
                        <div class="paper-img" id="deep-network-calibration"><img src="media/NCprob.png" /> <div><span class="paper-abstract">Abstract</span>: A technical challenge of deep learning is recognizing target classes without seen data. Zero-shot learning leverages semantic representations such as attributes or class prototypes to bridge source and target classes. Existing standard zero-shot learning methods may be prone to overfitting the seen data of source classes as they are blind to the semantic representations of target classes. In this paper, we study generalized zero-shot learning that assumes accessible to target classes for unseen data during training, and prediction on unseen data is made by searching on both source and target classes. We propose a novel Deep Calibration Network (DCN) approach towards this generalized zero-shot learning paradigm, which enables simultaneous calibration of deep networks on the confidence of source classes and uncertainty of target classes. Our approach maps visual features of images and semantic representations of class prototypes to a common embedding space such that the compatibility of seen data to both source and target classes are maximized. We show superior accuracy of our approach over the state of the art on benchmark datasets for generalized zero-shot learning, including AwA, CUB, SUN, and aPY.</div></div>
                    </li>

                    <li><div class="paper" target="condensenet">
                        <a class="paper-code" href="https://github.com/ShichenLiu/CondenseNet" target="_blank">[code]</a>
                        <a class="paper-talk" href="https://www.youtube.com/watch?v=NHajBzgnC1Q&t=87m50st" target="_blank">[talk]</a>
                        <a class="paper-title" href="https://arxiv.org/abs/1711.09224" target="_blank">CondenseNet: An Efficient DenseNet using Learned Group Convolutions</a> <br />
                        <span class="paper-author">Gao Huang*, <u>Shichen Liu</u>*, Laurens van der Maaten, Kilian Q. Weinberger</span> <br />
                        <span class="paper-conf">IEEE Conference on Computer Vision and Pattern Recognition (<span style="font-weight:bold">CVPR</span>) 2018, <span style="font-weight:bold">spotlight</span></span> <br />
                        </div>
                        <div class="paper-img" id="condensenet"><img src="media/learned_conv.png" /> <div style="margin-top:100px"><span class="paper-abstract">Abstract</span>: The high accuracy of convolutional networks (CNNs) in visual recognition tasks, such as image classification, has fueled the desire to deploy these networks on platforms with limited computational resources, e.g., in robotics, self-driving cars, and on mobile devices. Unfortunately, the most accurate deep CNNs, such as the winners of the ImageNet and COCO challenges, were designed without taking strict compute restrictions into consideration. As a result, these models cannot be used to perform real-time inference on low-compute devices.</div></div>
                    </li>

                    <li><div class="paper" target="DVSQ">
                        <a class="paper-code" href="https://github.com/caoyue10/cvpr17-dvsq" target="_blank">[code]</a>
                        <a class="paper-title" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Cao_Deep_Visual-Semantic_Quantization_CVPR_2017_paper.pdf" target="_blank">Deep Visual-Semantic Quantization for Efficient Image Retrieval</a> <br />
                        <span class="paper-author">Yue Cao, Mingsheng Long, <u>Shichen Liu</u>, Jianming Wang</span> <br />
                        <span class="paper-conf">IEEE Conference on Computer Vision and Pattern Recognition (<span style="font-weight:bold">CVPR</span>) 2017</span> <br />
                        </div>
                        <div class="paper-img" id="DVSQ"><div><span class="paper-abstract">Abstract</span>: Compact coding has been widely applied to approximate nearest neighbor search for large-scale image retrieval, due to its computation efficiency and retrieval quality. This paper presents a compact coding solution with a focus on the deep learning to quantization approach, which improves retrieval quality by end-to-end representation learning and compact encoding and has already shown the superior performance over the hashing solutions for similarity retrieval. We propose Deep Visual-Semantic Quantization (DVSQ), which is the first approach to learning deep quantization models from labeled image data as well as the semantic information underlying general text domains. The main contribution lies in jointly learning deep visual-semantic em- beddings and visual-semantic quantizers using carefully-designed hybrid networks and well-specified loss functions. DVSQ enables efficient and effective image retrieval by supporting maximum inner-product search, which is computed based on learned codebooks with fast distance table lookup. Comprehensive empirical evidence shows that DVSQ can generate compact binary codes and yield state-of-the-art similarity retrieval performance on standard benchmarks.</div>
                    </li>

                    <li><div class="paper" target="CDQ">
                        <a class="paper-code" href="https://github.com/caoyue10/aaai17-cdq" target="_blank">[code]</a>
                        <a class="paper-title" href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14499" target="_blank">Collective Deep Quantization for Efficient Cross-Modal Retrieval</a> <br />
                        <span class="paper-author">Yue Cao, Mingsheng Long, <u>Shichen Liu</u>, Jianming Wang</span> <br />
                        <span class="paper-conf">AAAI Conference on Artificial Intelligence (<span style="font-weight:bold">AAAI</span>) 2017</span> <br />
                        </div>
                        <div class="paper-img" id="CDQ"><div><span class="paper-abstract">Abstract</span>: Cross-modal similarity retrieval is a problem about designing a retrieval system that supports querying across content modalities, e.g., using an image to retrieve for texts. This paper presents a compact coding solution for efficient cross-modal retrieval, with a focus on the quantization approach which has already shown the superior performance over the hashing solutions in single-modal similarity retrieval. We propose a collective deep quan- tization (CDQ) approach, which is the first attempt to introduce quantization in end-to-end deep architecture for cross-modal retrieval. The major contribution lies in jointly learning deep representations and the quantizers for both modalities using carefully-crafted hybrid net- works and well-specified loss functions. In addition, our approach simultaneously learns the common quantizer codebook for both modalities through which the cross-modal correlation can be substantially enhanced. CDQ enables efficient and effective cross-modal retrieval using inner product distance computed based on the common codebook with fast distance table lookup. Extensive experiments show that CDQ yields state of the art cross-modal retrieval results on standard benchmarks.</div>
                    </li>
                </ul>
            </div>
            <div class="work">
                <h1><i class="fa fa-folder-open-o" aria-hidden="true"></i> Projects</h1>
                <h3><i class="fa fa-folder-open-o" aria-hidden="true"></i> Course projects</h3>
                <ul>
                    <li><a href="https://lustralisk.github.io/haskell-repl-compiler/" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> Lisp interpreter and compiler on Heskell [Compiler] (Functional Programming) </a></li>
                    <li><a href="https://github.com/Lustralisk/C-compiler" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> C to LLVM/Python compiler [Compiler] (Principle of Compiler, best project) </a></li>
                    <li><a href="https://github.com/Lustralisk/FTP-server" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> FTP Server [Server] (Computer Networking, best project) </a></li>
                    <li><a href="https://lmy1229.github.io/Carstructor/" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> Carstructor [Game] (Web Front-end Technology) </a></li>
                    <li><a href="https://lustralisk.github.io/game-of-life/" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> Cellular Automata (Software Engineering) </a></li>
                </ul>
            </div>
            <div class="award">
                <h1><i class="fa fa-trophy" aria-hidden="true"></i> Awards</h1>
                <h3><i class="fa fa-trophy" aria-hidden="true"></i> Rewards</h3>
                <ul>
                    <li> 3rd prize in 19th Intelligent body contest</li>
                    <li> Science and technology Scholarship, 2015</li>
                    <li> Science and technology Scholarship, 2016</li>
                    <li> Artistic Scholarship, 2016</li>
                    <li> Qualcomm Scholarship, 2016</li>
                    <li> Sensetime Scholarship, 2017</li>
                    <li> Best project in Web Front-end Technology course, Aug. 2016</li>
                    <li> Best project in Computer Networking course, Jan. 2017 </li>
                </ul>
                <h3><i class="fa fa-trophy" aria-hidden="true"></i> Achievements</h3>
                <ul>
                    <li>Master placement in <a href="https://starcraft2.com" target="_blank">StarCraft II</a> as Zerg, Diamond as Protoss</li>
                    <li>Platinum Cup of <a href="http://www.monsterhunterworld.com/" target="_blank">Monster Hunter: World</a></li>
                    <li>Platinum Cup of <a href="https://godofwar.playstation.com/" target="_blank">God of War</a></li>
                </ul>
            </div>
            <div class="article">
                <h1><i class="fa fa-bars" aria-hidden="true"></i> Blogs</h1>
                <h3><i class="fa fa-rss" aria-hidden="true"></i> Technical Blogs</h3>
                <ul>
                    <li><a href="https://github.com/Lustralisk/caffe-2-tensorflow" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> From Caffe to Tensorflow</a></li>
                    <li><a href="http://www.cnblogs.com/lustralisk/p/pythonProgressBar.html" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> How to implement a progression bar in python</a></li>
                    <li><a href="http://www.cnblogs.com/lustralisk/p/auctex.html" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> Guide on installation of auctex in emacs</a></li>
                </ul>
                <h3><i class="fa fa-pencil-square-o" aria-hidden="true"></i> Diary</h3>
                <ul>
                    <li><a href="blog/thinking.html" target_="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> My comprehension about Web front-end programming</a></li>
                </ul>
            </div>
            <div class="plan">
                <h1><i class="fa fa-calendar-o" aria-hidden="true"></i> Recently</h1>
            </div>
        </div>
        <a id="button" onclick="scroll_to_top()" style="text-decoration:none;color:#000;display:none;">
            <div class="top">
                <p>Back to top <i class="fa fa-arrow-circle-up" aria-hidden="true"></i></p>
            </div>
        </a>
        <script src="static/js/tool.js"></script>
    </body>
</html>

