<!DOCTYPE html>
<html>
    <head>
        <title>Shichen Liu's Homepage</title>
        <link rel="stylesheet" href="static/css/stylesheet.css" />
        <link rel="stylesheet" href="static/font-awesome-4.6.3/css/font-awesome.min.css" />
    </head>
    <body>
        <div class="col-3 nav">
            <a href="#"><img src="media/profile.jpg" class="img-circle" width="40%" /></a>
            <h1>Shichen Liu</h1>
            <p>Undergraduate of</p>
            <p>School of Software</p>
            <p><a href="http://www.tsinghua.edu.cn"><i class="fa fa-university" aria-hidden="true"></i> Tsinghua University</a></p>
            <p class="icon"><a href="http://github.com/shichenliu"><i class="fa fa-github" aria-hidden="true"></i></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="mailto:liushichen95@gmail.com"><i class="fa fa-envelope" aria-hidden="true"></i></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://www.facebook.com/shichen.liu.75"><i class="fa fa-facebook-square" aria-hidden="true"></i></a><p>
        </div>
        <div class="col-9 col-offset-3">
            <div class="bio">
                <h1><i class="fa fa-paperclip" aria-hidden="true"></i> About me</h1>
                <p>I am a senior undergraduate in the School of Software, Tsinghua University. I am currently in Machine Learning Group in <a href="http://ise.thss.tsinghua.edu.cn/" target="_blank">National Big Data Engineering Laboratory</a> in Tsinghua University under the supervision of <a href="http://ise.thss.tsinghua.edu.cn/~mlong/" target="_blank">Mingsheng Long</a>. I spent 12 wonderful weeks as research assistant in <a href="http://www.cs.cornell.edu/~kilian/index.html" target="_blank">Prof. Kilian Q. Weinberger</a>'s lab in Cornell University during the summer of 2017. I will be an intern researcher in visual computing group, MSRA, under the supervision of <a href="https://www.microsoft.com/en-us/research/people/jingdw/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fjingdw%2F">Jingdong Wang</a> from late 2017.</p> 

                Here is my <a href="media/cv_lsc.pdf" target="_blank">CV</a>.
            </div>
            <div class="paper-list">
                <h1><i class="fa fa-file-pdf-o" aria-hidden="true"></i> Papers</h1>
                <ul>
                    <li><div class="paper" target="condensenet">
                        <a class="paper-code" href="https://github.com/ShichenLiu/CondenseNet" target="_blank">[code]</a>
                        <a class="paper-title" href="https://arxiv.org/abs/1711.09224" target="_blank">CondenseNet: An Efficient DenseNet using Learned Group Convolutions</a> <br />
                        <span class="paper-author">Gao Huang*, Shichen Liu*, Laurens van der Maaten, Kilian Q. Weinberger</span> <br />
                        <span class="paper-conf">Prepaint 2018</span> <br />
                        </div>
                        <div class="paper-img" id="condensenet"><img src="media/learned_conv.png" /> <div style="margin-top:100px"><span class="paper-abstract">Abstract</span>: The high accuracy of convolutional networks (CNNs) in visual recognition tasks, such as image classification, has fueled the desire to deploy these networks on platforms with limited computational resources, e.g., in robotics, self-driving cars, and on mobile devices. Unfortunately, the most accurate deep CNNs, such as the winners of the ImageNet and COCO challenges, were designed without taking strict compute restrictions into consideration. As a result, these models cannot be used to perform real-time inference on low-compute devices.</div></div>
                    </li>

                    <li><div class="paper" target="NC">
                        <!-- <a class="paper-code" href="#" target="_blank">[code]</a> -->
                        <a class="paper-title" href="#" target="_blank">Zero-Shot Learning with Network Calibration</a> <br />
                        <span class="paper-author">Shichen Liu, Mingsheng Long, Jianming Wang</span> <br />
                        <span class="paper-conf">Prepaint 2018</span> <br />
                        </div>
                        <div class="paper-img" id="NC"><img src="media/NCprob.png" /> <div><span class="paper-abstract">Abstract</span>: Significant progress in object recognition has been made by deep learning from large-scale labeled data. However, the capability of deep learning to recognize unseen object classes without labeled data remains a technical challenge. Zero-shot learning is one way of addressing this challenge, which leverages semantic representations such as attributes and class prototypes to bridge seen and unseen classes. We discover a miscalibration problem underlying most existing zero-shot learning methods, which results in overfitting to the probabilistic likelihood and poor transferability on generalized zero-shot learning, where prediction is performed by searching on both seen and unseen classes. We propose a Network Calibration (NC) framework to enable temperature calibration and entropy calibration of deep networks, which jointly maps visual features of images and semantic representations of class prototypes to a common embedding space such that the compatibility of training data with both seen and unseen classes are maximized. Our new loss functions enable end-to-end training of deep networks by back-propagation. We demonstrate superior accuracy of our NC framework over the state of the art on benchmark datasets for zero-shot learning, including AwA, CUB, SUN, and aPY.</div></div>
                    </li>

                    <li><div class="paper" target="DVSQ">
                        <a class="paper-code" href="https://github.com/caoyue10/cvpr17-dvsq" target="_blank">[code]</a>
                        <a class="paper-title" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Cao_Deep_Visual-Semantic_Quantization_CVPR_2017_paper.pdf" target="_blank">Deep Visual-Semantic Quantization for Efficient Image Retrieval</a> <br />
                        <span class="paper-author">Yue Cao, Mingsheng Long, Jianming Wang, Shichen Liu</span> <br />
                        <span class="paper-conf">IEEE Conference on Computer Vision and Pattern Recognition (<span style="font-weight:bold">CVPR</span>) 2017</span> <br />
                        </div>
                        <div class="paper-img" id="DVSQ"><div><span class="paper-abstract">Abstract</span>: Compact coding has been widely applied to approximate nearest neighbor search for large-scale image retrieval, due to its computation efficiency and retrieval quality. This paper presents a compact coding solution with a focus on the deep learning to quantization approach, which improves retrieval quality by end-to-end representation learning and compact encoding and has already shown the superior performance over the hashing solutions for similarity retrieval. We propose Deep Visual-Semantic Quantization (DVSQ), which is the first approach to learning deep quantization models from labeled image data as well as the semantic information underlying general text domains. The main contribution lies in jointly learning deep visual-semantic em- beddings and visual-semantic quantizers using carefully-designed hybrid networks and well-specified loss functions. DVSQ enables efficient and effective image retrieval by supporting maximum inner-product search, which is computed based on learned codebooks with fast distance table lookup. Comprehensive empirical evidence shows that DVSQ can generate compact binary codes and yield state-of-the-art similarity retrieval performance on standard benchmarks.</div>
                    </li>

                    <li><div class="paper" target="CDQ">
                        <a class="paper-code" href="https://github.com/caoyue10/aaai17-cdq" target="_blank">[code]</a>
                        <a class="paper-title" href="https://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14499" target="_blank">Collective Deep Quantization for Efficient Cross-Modal Retrieval</a> <br />
                        <span class="paper-author">Yue Cao, Mingsheng Long, Jianming Wang, Shichen Liu</span> <br />
                        <span class="paper-conf">AAAI Conference on Artificial Intelligence (<span style="font-weight:bold">AAAI</span>) 2017</span> <br />
                        </div>
                        <div class="paper-img" id="CDQ"><div><span class="paper-abstract">Abstract</span>: Cross-modal similarity retrieval is a problem about designing a retrieval system that supports querying across content modalities, e.g., using an image to retrieve for texts. This paper presents a compact coding solution for efficient cross-modal retrieval, with a focus on the quantization approach which has already shown the superior performance over the hashing solutions in single-modal similarity retrieval. We propose a collective deep quan- tization (CDQ) approach, which is the first attempt to introduce quantization in end-to-end deep architecture for cross-modal retrieval. The major contribution lies in jointly learning deep representations and the quantizers for both modalities using carefully-crafted hybrid net- works and well-specified loss functions. In addition, our approach simultaneously learns the common quantizer codebook for both modalities through which the cross-modal correlation can be substantially enhanced. CDQ enables efficient and effective cross-modal retrieval using inner product distance computed based on the common codebook with fast distance table lookup. Extensive experiments show that CDQ yields state of the art cross-modal retrieval results on standard benchmarks.</div>
                    </li>
                </ul>
            </div>
            <div class="work">
                <h1><i class="fa fa-folder-open-o" aria-hidden="true"></i> Projects</h1>
                <h3><i class="fa fa-folder-open-o" aria-hidden="true"></i> Course projects</h3>
                <ul>
                    <li><a href="https://lustralisk.github.io/haskell-repl-compiler/" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> Lisp interpreter and compiler on Heskell [Compiler] (Functional Programming) </a></li>
                    <li><a href="https://github.com/Lustralisk/C-compiler" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> C to LLVM/Python compiler [Compiler] (Principle of Compiler, best project) </a></li>
                    <li><a href="https://github.com/Lustralisk/FTP-server" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> FTP Server [Server] (Computer Networking, best project) </a></li>
                    <li><a href="https://lmy1229.github.io/Carstructor/" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> Carstructor [Game] (Web Front-end Technology) </a></li>
                    <li><a href="https://lustralisk.github.io/game-of-life/" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> Cellular Automata (Software Engineering) </a></li>
                </ul>
            </div>
            <div class="award">
                <h1><i class="fa fa-trophy" aria-hidden="true"></i> Awards</h1>
                <h3><i class="fa fa-trophy" aria-hidden="true"></i> Rewards</h3>
                <ul>
                    <li> 3rd prize in 19th Intelligent body contest</li>
                    <li> Science and technology Scholarship, 2015</li>
                    <li> Science and technology Scholarship, 2016</li>
                    <li> Artistic Scholarship, 2016</li>
                    <li> Qualcomm Scholarship, 2016</li>
                    <li> Sensetime Scholarship, 2017</li>
                    <li> Best project in Web Front-end Technology course, Aug. 2016</li>
                    <li> Best project in Computer Networking course, Jan. 2017 </li>
                </ul>
                <h3><i class="fa fa-trophy" aria-hidden="true"></i> Achievements</h3>
                <ul>
                    <li>Diamond placement in StarCraft II as Zerg</li>
                </ul>
            </div>
            <div class="article">
                <h1><i class="fa fa-bars" aria-hidden="true"></i> Blogs</h1>
                <h3><i class="fa fa-rss" aria-hidden="true"></i> Technical Blogs</h3>
                <ul>
                    <li><a href="https://github.com/Lustralisk/caffe-2-tensorflow" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> From Caffe to Tensorflow</a></li>
                    <li><a href="http://www.cnblogs.com/lustralisk/p/pythonProgressBar.html" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> How to implement a progression bar in python</a></li>
                    <li><a href="http://www.cnblogs.com/lustralisk/p/auctex.html" target="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> Guide on installation of auctex in emacs</a></li>
                </ul>
                <h3><i class="fa fa-pencil-square-o" aria-hidden="true"></i> Diary</h3>
                <ul>
                    <li><a href="blog/thinking.html" target_="_blank"><i class="fa fa-external-link" aria-hidden="true"></i> My comprehension about Web front-end programming</a></li>
                </ul>
            </div>
            <div class="plan">
                <h1><i class="fa fa-calendar-o" aria-hidden="true"></i> Recently</h1>
            </div>
        </div>
        <a id="button" onclick="scroll_to_top()" style="text-decoration:none;color:#000;display:none;">
            <div class="top">
                <p>Back to top <i class="fa fa-arrow-circle-up" aria-hidden="true"></i></p>
            </div>
        </a>
        <script src="static/js/tool.js"></script>
    </body>
</html>

